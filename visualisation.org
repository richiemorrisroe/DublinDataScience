#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [presentation]
#+BEAMER_THEME: Berkeley
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 :ETC
#+BIND: org-latex-image-default-width 0.7\linewidth
#+BIND: org-latex-image-default-height 0.7\linewidth
#+PROPERTY: header-args:R :session *R*  :eval no-export :height 360 :width 360
#+OPTIONS: tasks:nil
#+OPTIONS: toc:nil
#+AUTHOR: Richie Morrisroe
#+TITLE: Visualisation: Modelling the World

* Structure
- This talk is an approach to visualisation
- Not many absolutes
- assumptions of vision
- Assumptions of Statistical Graphics
- Understanding data with Visualisation
- Communicating to others with Visualisation
* What is Visualisation?
- a tool for understanding the world
- a way to communicate a particular perspective on data
- an adjunct to thought
* Why Visualisation?
- The eye is really really good at finding patterns in pictures
- in fact, it's so good that  it can find patterns that aren't even
there
#+CAPTION: What do you see?
#+NAME: fig:old_young
#+attr_latex: :width 100px :height 100px
#+attr_html: :width 100px :height 100px
[[./old_young.png]]m 
* The importance of perspective
- You can see one of two things in the previous image
- Which of them can depend on what you expect to see
- It can also depend on what your environment contains
* Muller-Lyer 
#+CAPTION: Which line is longer?
#+NAME: muller_lyer
#+attr_latex: :width 100px :height 100px
[[./muller_lyer.png]]

* This illusion doesn't affect everyone similarly
- Europeans and Americans are more susceptible
- Africans are less susceptible
- Possibility that it is due to presence of right angles in urban environments
- appears to be a small difference between urban and rural dwellers
* Who cares?
- Shows that how we interpret stimuli is not *tabula rasa*
- When you gaze into the image, the image also gazes into you...
- We bring our own perception and previous associations into any image [fn:1]
* When to use Visualisation?
# should be in massive text
#+BEGIN_LaTeX
\begin{center}
 {\Huge Always}  
\end{center}

#+END_LaTeX

* Running Example
- Property Price Register
  - Kinda a crappy dataset
  - No cleaning or checking done by the authority 
  - lots of craziness (1 apartment for 18.6mn)
* Property Price Register
- We used Google's geocoding service to get more details on each observation
- I updated [[https://www.shanelynn.ie/tag/ppr/][Shane Lynn's]] script and ran it on the data up till October 2018
- I also typically break out properties sold for greater than 1e6, as
  they are often multiple-unit sales (and there's little to no
  automated way of figuring this out) [fn:4]
- Lots of manual fixing required
- the irish text definitely doesn't help
* Assumptions of Statistical Graphics
- there are many
- in this section, I'd like to subvert them, in order to make you think
* Line Graphs
- Normally represent time
- scatterplots don't (always) have the same assumptions
- what is the deepest assumption?
* Median Property Price by Day, Ireland 2011-18
#+BEGIN_SRC R :session  :results none :exports none 
require(sp)
require(rgdal)
require(tidyverse)
ppr_gc <- read_csv("~/Dropbox/PPR/ppr_geocoded_till_oct2018.csv")

ppr_gc_smaller <- select(ppr_gc, year, input_string, sale_date, price, ppr_county, geo_county, description_of_property, 15:24)
ppr_gc_less_than_1m <- filter(ppr_gc_smaller, price<2e6)
ppr_gc2 <- filter(ppr_gc_less_than_1m, !is.na(latitude), !is.na(electoral_district))
locs <- select(ppr_gc2, longitude, latitude)
sp_ppr <- SpatialPointsDataFrame(locs, data=ppr_gc2, proj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
shp <- readOGR("~/Dropbox/PPR/electoral_divisions_gps.shp")
dublin_counties <- c("Fingal", "Dn Laoghaire-Rathdown", "Dublin City", 
                     "South Dublin", "Kildare County", "Wicklow County")
dubcity <- "Dublin City"
duball <- shp[as.character(shp@data$COUNTYNAME) %in% 
              dublin_counties, ]
dubcity <- shp[as.character(shp@data$COUNTYNAME)=="Dublin City",]

dubcity <- filter(ppr_gc2, geo_county %in% dublin_counties)
dubcity_samp <- sample_frac(dubcity, size=0.3)
#+END_SRC



#+BEGIN_SRC R :session :results none :exports none
ppr_gc3 <- ppr_gc2 %>% mutate(is_dublin=ifelse(ppr_county=="Dublin", "Yes", "No"))
median_price_by_day <- ppr_gc2 %>% group_by(sale_date) %>%
    summarise(count=n(),
              median_price=median(price, na.rm=TRUE))

median_price_by_day <- ppr_gc2 %>% mutate(is_dublin=ifelse(ppr_county=="Dublin", 1, 0)) %>%
group_by(sale_date,is_dublin) %>%
    summarise(count=n(),
              median_price=median(price, na.rm=TRUE))
median_price_by_day_reversed <-
    mutate(median_price_by_day, date_reverse=rev(sale_date),
           price_reverse=rev(median_price))

#+END_SRC


#+BEGIN_SRC R :session :results output graphics :file line1.png :exports results
regular_line <- ggplot(median_price_by_day, aes(x=sale_date, y=median_price))+geom_line()+geom_smooth()
print(regular_line)
#+END_SRC
#+RESULTS:
[[file:line1.png]]
* Flipped Line Chart

#+BEGIN_SRC R :session :results output graphics :file line2.png :exports results
require(gridExtra)
flipped_line <- ggplot(median_price_by_day, aes(x=sale_date, y=median_price))+geom_line()+coord_flip()
print(flipped_line+geom_smooth())

#+END_SRC

#+ATTR_LATEX: :width .9\linewidth :height .9\textheight
#+RESULTS:
[[file:line2.png]]
* F-ing Line Chart

#+begin_src R :session :results output graphics :file line5.png :exports results 
ggplot(median_price_by_day, aes(y=sale_date, x=median_price))+geom_line()  
#+end_src

#+RESULTS:
[[file:line5.png]]

- Here, the violence is that we swap the axes in a fashion only a monster would
* Abusing Standard Assumptions

#+begin_src R :session :results output graphics :file line4.png :exports results
  ggplot(median_price_by_day, aes(y=sale_date, x=median_price))+geom_line()+geom_smooth()
#+end_src

  #+RESULTS:
  [[file:line4.png]]

* Backwards Line Chart :noexport:
#+BEGIN_SRC R :session :results output graphics :file line3.png :exports results  
ggplot(median_price_by_day_reversed, aes(x=1:nrow(median_price_by_day_reversed), y=price_reverse))+geom_line()
#+END_SRC

#+RESULTS:
[[file:line3.png]]


- The only way to get this to work is to do violence to the intention
  of the tool
* Scatter plot
- Also encodes a set of base assumptions
- points nearer to each other in space are more related
- more orientation issues
* Standard Scatter
#+BEGIN_SRC R :session :results output graphics :exports results :file scatter1.png 

ggplot(median_price_by_day,
       aes(x=median_price, y=count))+geom_point()
#+END_SRC

#+RESULTS:
[[file:scatter1.png]]
* Flipped Scatter

#+BEGIN_SRC R :session :results output graphics :exports results :file scatter2.png 

ggplot(median_price_by_day,
       aes(x=median_price, y=count))+geom_point()+coord_flip()
#+END_SRC

#+RESULTS:
[[file:scatter2.png]]
* Other side

#+BEGIN_SRC R :session :results output graphics :exports results :file scatter3.png 

price_count_negative <- select(median_price_by_day, median_price, count) %>%
    mutate(price2=-1*median_price, count2=-1*count)
ggplot(price_count_negative,
       aes(x=price2, y=count2))+geom_point()
#+END_SRC

#+RESULTS:
[[file:scatter3.png]]


* What does this tell us?
- We have a base level of assumptions that we bring to graphics (especially statistical graphics)
- Most of these appear to have been formed by Descartes 
- When these assumptions are subverted, expect problems
* Simple Statistical Graphics
- Graphs excel at showing relations between things
- Consider the difference between quantiles of a variable, and a density plot
- For example, the price of houses:
#+begin_src R :session :colnames no :rownames yes :exports results
with(ppr_gc, quantile(price, seq(0, 1, .1))) %>% as.data.frame() 
#+end_src

#+RESULTS:
|   0% |      5079 |
|  10% |     55000 |
|  20% |     85000 |
|  30% |    115000 |
|  40% |    145000 |
|  50% |    175000 |
|  60% |    214000 |
|  70% |    255505 |
|  80% |    315000 |
|  90% |    430000 |
| 100% | 139165000 |
* Density Plot
  #+begin_src R :session :results output graphics :file dens1.png :exports results
ggplot(ppr_gc, aes(x=price))+geom_density()
  #+end_src

  #+RESULTS:
  [[file:dens1.png]]
* Better Density Plot
#+begin_src R :session :results output graphics :exports results :file dens2.png   
ggplot(ppr_gc, aes(x=log(price, 10)))+geom_density()
#+end_src
  

  #+RESULTS:
  [[file:dens2.png]]
* Transformations
- Useful to get a better sense of the data
- Have a bunch of assumptions (what's the log of -1)
- Can be used to deceive very, very easily
- Really really useful in everyday practice
* Getting the sense of things
- Picking the right visualisation for the data is important

#+begin_src R :session :results output graphics :file scatter_bad.png  :exports results
ggplot(ppr_gc, aes(x=sale_date, y=price))+geom_point()
#+end_src


#+RESULTS:
[[file:scatter_bad.png]]

- is this a good plot?
- does this depend on the number of points?
* Cleaning the Data
- Let's say we remove all properties with prices greater than 2mn
#+begin_src R :session :results output graphics :file scatter_bad2.png  :exports results
ggplot(ppr_gc2, aes(x=sale_date, y=price))+geom_point()+scale_y_continuous(labels=scales::dollar_format(prefix="€" ))
#+end_src

#+RESULTS:
[[file:scatter_bad2.png]]
* More Data Cleaning
#+BEGIN_SRC R :session :results output graphics :file scatter_bad3.png :exports results
ggplot(ppr_gc2, aes(x=sale_date, y=price))+geom_point()+coord_cartesian(ylim=c(0, 1e6))+scale_y_continuous(labels=scales::dollar_format(prefix="€" ))
#+END_SRC

#+RESULTS:
[[file:scatter_bad3.png]]
- Better or worse?
* Sampling and Plotting
#+begin_src R :session :results output graphics :file scatter_bad2.png :exports results
ggplot(dubcity_samp, aes(x=sale_date, y=price))+geom_point()+scale_y_continuous(labels=scales::dollar_format(prefix="€" ))
#+end_src

#+RESULTS:
[[file:scatter_bad2.png]]

- Not really
* Transformations Help 
  #+begin_src R :session :results output graphics :file logscatter.png :exports results 
  ggplot(ppr_gc_smaller, aes(x=sale_date, y=log(price, 10)))+geom_point()
  #+end_src

    #+RESULTS:
    [[file:logscatter.png]]

- Note the log 10 base
- Some of you may be able to convert from base 2.718, but I missed
  that class in school
- Still crap though
* No data is an island

- The first obvious thing is to split by county, right?
#+begin_src R :session :results output graphics :file scat_county1.png :exports results
ggplot(ppr_gc_smaller, aes(x=sale_date, y=log(price, 10)))+geom_point()+facet_wrap(~ppr_county)
#+end_src


#+RESULTS:
[[file:scat_county1.png]]
- Oh look, it's lot of little boxes of crap :(
* Summarisation
- The obvious answer is summarisation
#+begin_src R :session :exports both :results output graphics :file linecounty1.png 
county_daily <- ppr_gc2 %>% group_by(sale_date, ppr_county, region) %>%
  summarise(count=n(), min_price=min(price),
            median_price=median(price),
            max_price=max(price)) %>%
  mutate(min_to_median=min_price/median_price,
         max_to_median=max_price/median_price,
         max_to_min=max_price/min_price)
ggplot(county_daily, aes(x=sale_date, y=median_price, colour=ppr_county))+geom_line()+theme(legend.position="none")
#+end_src

#+RESULTS:
[[file:linecounty1.png]]



* Reducing Alpha kinda works...
  #+begin_src R :session :results output graphics :file linecounty2.png :exports results
  ggplot(county_daily, aes(x=sale_date, y=median_price, colour=ppr_county))+geom_line(alpha=0.3)+theme(legend.position="none")
  #+end_src

  #+RESULTS:
  [[file:linecounty2.png]]

- But really just washes the whole thing out
* A redundant faceting variable
- We just group by a higher level variable
#+begin_src R :session :results output graphics :file linecounty3.png :exports results
ggplot(county_daily, aes(x=sale_date, y=median_price, colour=ppr_county))+geom_line()+facet_wrap(~region)
#+end_src

#+RESULTS:
[[file:linecounty3.png]]

- Much clearer :)
* WTF?
- This is one of the major advantages of visualisation:
  - it helps to (dis)confirm your assumptions
  - given that we have too many lines in the various groupings,we know
    that somethng has gone horribly wrong
  - in this case, it's a mismatch between two different types of data
* Iterating over data and visuals :noexport:
y#+begin_src R :session :colnames yes :eval no
# this is an S4 object with geographical data 
shp <- readOGR("~/Dropbox/PPR/electoral_divisions_gps.shp") 
#the data slot contains a dataframe - countyname is the LEO's
with(shp@data, table(COUNTYNAME)) %>% as.data.frame() %>% arrange(desc(Freq)) %>% head(10)
#+end_src

#+RESULTS:
| COUNTYNAME      | Freq |
|-----------------+------|
| Cork County     |  324 |
| Galway County   |  214 |
| Kerry County    |  164 |
| Dublin City     |  162 |
| Mayo County     |  152 |
| Clare County    |  151 |
| Donegal County  |  149 |
| Limerick County |  135 |
| Wexford County  |  124 |
| Kilkenny County |  113 |
- These are the local electoral authorities
- These are from the geocoded points, so they should be somewhat better
- the PPR data is sometimes crazy wrong [fn:5]


#+begin_src R :session :results none :exports code
#NUTS3
county_region_map <- shp@data[,"COUNTYNAME", "NUTS3NAME"]
ppr_gc_county_fix <- ppr_gc2 %>%
  mutate(COUNTYNAME=ifelse(length(geo_county)==1, paste(geo_county, "County", sep=" "), geo_county))
  
#+end_src
* Distributions (i.e. boxplots)
#+begin_src R :session :results output graphics :file boxplot1-0.png :exports results :width 240 :height 240
  ggplot(ppr_gc2, aes(x=as.factor(year), y=price))+geom_boxplot()
#+end_src

  #+RESULTS:
  [[file:boxplot1-0.png]]
* Faceting, redux
  #+begin_src R :session :results output graphics :file boxplot2.png :exports results  
ggplot(ppr_gc2, aes(x=as.factor(year), y=price))+geom_boxplot()+facet_wrap(~region)
  #+end_src
    #+RESULTS:
    [[file:boxplot2.png]]

- This actually works (for me, at least)
- can you explain this to a sales-person?

  

* Distributions over Time, Redux
  #+begin_src R :session :results output graphics :file density_year.png :exports results :width 400 :height 400 :center yes
  ggplot(ppr_gc2, aes(x=log(price, 10), fill=region))+geom_density(alpha=0.3)+facet_wrap(~year)+xlab("log10_price")+theme(axis.text.x=element_text(angle=-45))
  #+end_src
  #+RESULTS:
  [[file:density_year.png]]
- This is much, much better
- I definitely don't think I'd try to explain it to a business/sales person
* Spatial vs Temporal
- line plots vs maps
- time versus space
- both provide insight into 
- pick one, difficult to do both
* Line plots ignore space, maps ignore time
#+BEGIN_SRC R :exports none :results none
require(sf)
require(rgeos)
tenure <- read_csv("~/Dropbox/PPR/housing_tenure.csv") %>% normalise_names()
names(tenure) <- gsub("^_", "perc_", x=names(tenure))

ppr_tenure_m <- merge(ppr_gc, tenure, by.x="small_area", by.y="geog_id", all.x=TRUE)
ppr_tenure_less_1m <- filter(ppr_tenure_m, price<=1e6)
ppr_tenure_more_1m <- filter(ppr_tenure_m, price>1e6)
elec_price <- ppr_tenure_less_1m %>% group_by(electoral_district, electoral_district_id, year) %>% summarise(med_price=median(price), count=n())
elec_m <- merge(dub_counties, elec_price, by.x="CSOED", by.y="electoral_district_id", duplicateGeoms=TRUE)
# elec_m_duball <- merge(subset, elec_price, by.x="CSOED", by.y="electoral_district_id", duplicateGeoms=TRUE)

elec_m_sf <- st_as_sf(elec_m)
elec_m_tenure <- merge(elec_m_sf, tenure, by.x="CSOED", by.y="ed_ward_id")
#this took more time than I expected. 
require(sf)
subset_sf <- st_as_sf(dub_counties)
subset_sf2 <- mutate(subset_sf, PROP_UNOCC=UNOCC2011/HS2011, PROP_MALE=MALE2011/TOTAL2011, PROP_FEMALE=FEMALE2011/TOTAL2011, DENSITY=TOTAL2011/LAND_AREA, PEOPLE_PER_HS=TOTAL2011/HS2011)
#+END_SRC

#+BEGIN_SRC R :session :results output graphics :file map1.png :exports results 
ggplot(elec_m_sf, aes(fill=med_price))+geom_sf()
#+END_SRC

#+RESULTS:
[[file:map1.png]]
- There's a real problem of scale here, in that Dublin City is both
  responsible for much of th population, but is invisible
-
* Dirty Oul Town
#+BEGIN_SRC R :session :results output graphics :exports results :file map2.png
filter(elec_m_sf, COUNTYNAME=="Dublin City") %>% ggplot( aes(fill=med_price))+geom_sf()
#+END_SRC

#+RESULTS:
[[file:map2.png]]
* Counts tell a different story
#+BEGIN_SRC R :session :results output graphics :exports results :file map3.png
ggplot(elec_m_sf, aes(fill=count))+geom_sf()
#+END_SRC

#+RESULTS:
[[file:map3.png]]
- Outliers make the map useless

* Dublin City (again)
#+BEGIN_SRC R :session :results output graphics :exports results map4.png
filter(elec_m_sf, COUNTYNAME=="Dublin City") %>% ggplot(elec_m_sf, aes(x=count))+geom_sf()
#+END_SRC

* Density Plots to help maps
#+BEGIN_SRC R :session :results output graphics :exports results :file dens_both.png
duball_p <- ggplot(elec_m_sf, aes(x=count))+geom_density()
dubcity_p  <- filter(elec_m_sf, COUNTYNAME=="Dublin City") %>%
    ggplot( aes(x=count))+geom_density()
print(grid.arrange(duball_p, dubcity_p))
#+END_SRC

#+RESULTS:
[[file:dens_both.png]]

- A tiny proportion of electoral districts drive the uselessness of the maps
* Maps over Time
#+BEGIN_SRC R :session :results output graphics :file map6.png :exports results
filter(elec_m_sf, COUNTYNAME=="Dublin City", year>=2016) %>% ggplot(aes(fill=count))+geom_sf()+facet_wrap(~year)
#+END_SRC

#+RESULTS:
[[file:map6.png]]
- Just doesn't work
* Lines for Time

#+BEGIN_SRC R :session :results output graphics :exports results :file line_time.png
ggplot(elec_m_sf, aes(x=year, y=count, colour=electoral_district))+geom_line()+theme(legend.position="none")+geom_smooth()
#+END_SRC

#+RESULTS:
[[file:line_time.png]]
- This shows the trend plus outliers
- Much more useful
- lose the spatial dimension
* Interactivity and Dashboards
- Can show both time and space
- for reporting, these are essential
- Much more effort from a software-engineering perspective [fn:3]
* Performative vs Presentation
- Two types of graphs:
  - for yourself
  - for other people  (and different audiences need different things)
* Performative Graphics
- These are used to help you understand a problem
- typically created in an iterative fashion
- often move from data transformation to visualisation and back again (like this talk)
* How to visualise common types of data :noexport:
- scatterplot
- line plot
- reversed line plot (time moves from RTL)
- box plot 
- reversed box plot
* Presentation Graphs
- To some extent, your job with presentation visualisations is to tell a story
- hopefully, it will be nuanced, but that isn't a requirement [fn:2]
- Often good to show smooths as opposed to raw data
- raw data is often ugly
- need for care here, as this should only be done where there is a
  clear effect

* Advice
- As few as possible
- One clear message
- Repeat yourself
- Remove nuance
* As few as possible
- There should be no extraneous graphs 
- Each graph should have a clear purpose
- Smooths are really effective
* One Clear Message
- You should only be telling one story at a time
- People are easily confused
- Especially in an oral presentation
- Backup docs should contain nuance
* Repeat Yourself
- This is the key to helping people retain information
- This is easier once you know the story
- Say what you want to say, say it, then say what you said
* Remove Nuance
- This varies by audience
- Salespeople may just want the results
- colleagues may want to see the code
- most people just want a high level explanation
- Nuance should be present, just not in a presentation
* Conclusions
- Everyone bring assumptions to visualisations
- Make sure that you take advantage of this
- Visualisation is primarily a tool for communicating with yourself
- Iterative process, even bad graphs can teach you something
- Secondarily, it's a tool for communicating with others
- When using visualisations with others, keep it simple
* More Info
- My property article [[http://richiemorrisroe.github.io/property/PPR.html][here]]
- My repository for [[https://github.com/richiemorrisroe/DublinDataScience][this talk]]
- My crazy long notes file with [[https://github.com/richiemorrisroe/PPR][most of my analyses]]
- the data [[https://propertypriceregister.ie/website/npsra/pprweb.nsf/PPR?OpenForm][itself]]

* Reporting :noexport:
- Some times you need to repeat yourself
- Couple of ways of approaching this
  - Dashboards
  - Automated Reports
** Dashboards
- Lots of effort to set up correctly
- typically need a bunch of ETL to get data into correct format
- Low-maintenance once the original work is done
- Much more useful for business users 
** Automated Reports
- Less effort to get working (especially with Sweave, knitr and org/pandoc)
- A lot more effort to get working in a Python/SQL context
- More maintenance over time (someone needs to update the report)
** Principles of Reporting Visualisations
- Time view essential
- preferably forecasts, with results of previous forecasts
- allows 
- Simple, simple, simple
- One clear message (key metric or whatever)
- available material for those that want to dig deeper

* sessionInfo
#+BEGIN_SRC R :session :exports results :results verbatim
print(sessionInfo())
#+END_SRC

#+RESULTS:

  
* Footnotes

[fn:5] one wonders if that's deliberate 

[fn:4] please someone in the audience suggest a better idea 

[fn:1] anything really, but we're talking about images here. 

[fn:2] and in fact, it may be better to remove all nuance from the
presentation and provide a longer document with all the failed
approaches and hacking needed to actually reproduce your results

[fn:3] for me, at least


