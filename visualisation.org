#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js@3.7.0
#+PROPERTY: header-args 
#+PROPERTY: header-args:R :session *R*  :eval no-export :width 300 :height 300
#+OPTIONS: tasks:nil
#+OPTIONS: toc:nil
#+AUTHOR: Richie Morrisroe
#+TITLE: Visualisation: Modelling the World


* What is Visualisation?
- a tool for understanding the world
- a way to communicate a particular perspective on data
- an adjunct to thought
* Why Visualisation?
- The eye is really really good at finding patterns in pictures
- in fact, it's so good that  it can find patterns that aren't even
there
#+CAPTION: What do you see?
#+NAME: fig:old_young
#+attr_latex: :width 100px :height 100px
#+attr_html: :width 100px :height 100px
[[./old_young.png]]
* The importance of perspective
- You can see one of two things in the previous image
- Which of them can depend on what you expect to see
- It can also depend on what your environment contains
* Muller-Lyer 
#+CAPTION: Which line is longer?
#+NAME: muller_lyer
#+attr_latex: :width 100px :height 100px
#+attr_html: :width 100px :height 100px
[[./muller_lyer.png]]

* This illusion doesn't affect everyone similarly
- Europeans and Americans are more susceptible
- Africans are less susceptible
- Possibility that due to presence of right angles in urban environments
- appears to be a small difference between urban and rural dwellers
* Who cares?
- Shows that how we interpret stimuli is not *tabula rasa*
- When you gaze into the image, the image also gazes into you...
- We bring our own perception and previous associations into any image [fn:1]
* When to use Visualisation?
# should be in massive text
#+ATTR_LATEX: \Huge
#+begin_export latex
\center {\Huge Always}
#+end_export

* Running Example
- Property Price Register
  - Kinda a crappy dataset
  - No cleaning or checking done by the authority 
  - lots of craziness (1 apartment for 18.6mn)
* Property Price Register
- We used Google's geocoding service to get more details on each observation
- I updated [[https://www.shanelynn.ie/tag/ppr/][Shane Lynn's]] script and ran it on the data up till October 2018
- I also typically break out properties sold for greater than 1e6, as
  they are often multiple-unit sales (and there's little to no
  automated way of figuring this out) [fn:4]
- Lots of manual fixing required
- the irish text definitely doesn't help
* Assumptions of Statistical Graphics
- there are many
- in this section, I'd like to subvert them, in order to make you think
* Line Graphs
- Normally represent time
- scatterplots don't (always) have the same assumptions
- what is the deepest assumption?
* Median Property Price by Day, Ireland 2011-18
#+BEGIN_SRC R :session  :results none :exports none 
require(sp)
require(rgdal)
require(tidyverse)
ppr_gc <- read_csv("~/Dropbox/PPR/ppr_geocoded_till_oct2018.csv")

ppr_gc_smaller <- select(ppr_gc, year, input_string, sale_date, price, ppr_county, geo_county, description_of_property, 15:24)
ppr_gc_less_than_1m <- filter(ppr_gc_smaller, price<1e6)
ppr_gc2 <- filter(ppr_gc_less_than_1m, !is.na(latitude), !is.na(electoral_district))
locs <- select(ppr_gc2, longitude, latitude)
sp_ppr <- SpatialPointsDataFrame(locs, data=ppr_gc2, proj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
shp <- readOGR("~/Dropbox/PPR/electoral_divisions_gps.shp")
dublin_counties <- c("Fingal", "Dn Laoghaire-Rathdown", "Dublin City", 
                     "South Dublin", "Kildare County", "Wicklow County")
dubcity <- "Dublin City"
duball <- shp[as.character(shp@data$COUNTYNAME) %in% 
              dublin_counties, ]
dubcity <- shp[as.character(shp@data$COUNTYNAME)=="Dublin City",]

dubcity <- filter(ppr_gc2, geo_county %in% dublin_counties)
dubcity_samp <- sample_frac(dubcity, size=0.3)
#+END_SRC



#+BEGIN_SRC R :session :results none :exports none
ppr_gc3 <- ppr_gc2 %>% mutate(is_dublin=ifelse(ppr_county=="Dublin", "Yes", "No"))
median_price_by_day <- ppr_gc2 %>% group_by(sale_date) %>%
    summarise(count=n(),
              median_price=median(price, na.rm=TRUE))

median_price_by_day <- ppr_gc2 %>% mutate(is_dublin=ifelse(ppr_county=="Dublin", 1, 0)) %>%
group_by(sale_date,is_dublin) %>%
    summarise(count=n(),
              median_price=median(price, na.rm=TRUE))
median_price_by_day_reversed <-
    mutate(median_price_by_day, date_reverse=rev(sale_date),
           price_reverse=rev(median_price))

#+END_SRC


#+BEGIN_SRC R :session :results output graphics :file line1.png :exports results
regular_line <- ggplot(median_price_by_day, aes(x=sale_date, y=median_price))+geom_line()
#+END_SRC
#+attr_latex: :width 100px :height 100px
#+attr_html: :width 100px :height 100px
#+RESULTS:
[[file:line1.png]]
* Flipped Line Chart

#+BEGIN_SRC R :session :results output graphics :file line2.png :exports results
flipped_line <- ggplot(median_price_by_day, aes(x=sale_date, y=median_price))+geom_line()+coord_flip()
grid.arrange(regular_line+geom_smooth(), flipped_line+geom_smooth())

#+END_SRC

#+attr_latex: :width 200px :height 200px
#+attr_html: :width 100px :height 100px
#+RESULTS:
[[file:line2.png]]
* F-ing Line Chart

  #+begin_src R :session :results output graphics :file line5.png
  
ggplot(median_price_by_day, aes(y=sale_date, x=median_price))+geom_line()  
#+end_src
#+attr_latex :width 150px :height 150px
#+RESULTS:
[[file:line5.png]]

- Here, the violence is that we swap the axes in a fashion only a monster would
* Abusing Standard Assumptions

  #+begin_src R :session :results output graphics :file line4.png
  ggplot(median_price_by_day, aes(y=sale_date, x=median_price))+geom_line()+geom_smooth()
  #+end_src

  #+RESULTS:
  [[file:line4.png]]

* Backwards Line Chart
#+BEGIN_SRC R :session :results output graphics :file line3.png :exports results 
ggplot(median_price_by_day_reversed, aes(x=1:nrow(median_price_by_day_reversed), y=price_reverse))+geom_line()
#+END_SRC

#+RESULTS:
[[file:line3.png]]


- The only way to get this to work is to do violence to the intention
  of the tool
* Scatter plot
- Also encodes a set of base assumptions
- points nearer to each other in space are more related
- more orientation issues
* Standard Scatter
#+BEGIN_SRC R :session :results output graphics :exports results :file scatter1.png
ggplot(median_price_by_day,
       aes(x=median_price, y=count))+geom_point()
#+END_SRC

#+RESULTS:
[[file:scatter1.png]]
* Flipped Scatter

#+BEGIN_SRC R :session :results output graphics :exports results :file scatter2.png
ggplot(median_price_by_day,
       aes(x=median_price, y=count))+geom_point()+coord_flip()
#+END_SRC

#+RESULTS:
[[file:scatter2.png]]
q[[file:scatter2.png]]w

#+BEGIN_SRC R :session :results output graphics :exports results :file scatter3.png
price_count_negative <- select(median_price_by_day, median_price, count) %>%
    mutate(price2=-1*median_price, count2=-1*count)
ggplot(price_count_negative,
       aes(x=price2, y=count2))+geom_point()
#+END_SRC

#+RESULTS:
[[file:scatter3.png]]

* What does this tell us?
- We have a base level of assumptions that we bring to graphics (especially statistical graphics)
- Most of these appear to have been formed by Descartes 
- When these assumptions are subverted, expect problems
* Simple Statistical Graphics
- Graphs excel at showing relations between things
- Consider the difference between quantiles of a variable, and a density plot
- For example, the price of houses:
#+begin_src R :session :colnames no :rownames yes :exports results
with(ppr_gc, quantile(price, seq(0, 1, .1))) %>% as.data.frame() 
#+end_src

#+RESULTS:
|   0% |      5079 |
|  10% |     55000 |
|  20% |     85000 |
|  30% |    115000 |
|  40% |    145000 |
|  50% |    175000 |
|  60% |    214000 |
|  70% |    255505 |
|  80% |    315000 |
|  90% |    430000 |
| 100% | 139165000 |
* Density Plot
  #+begin_src R :session :results output graphics :file dens1.png :exports results
ggplot(ppr_gc, aes(x=price))+geom_density()
  #+end_src
  #+attr_latex :width 150px :height 150px
  #+RESULTS:
  [[file:dens1.png]]
* Better Density Plot
  #+begin_src R :session :results output graphics :exports results :file dens2.png
  ggplot(ppr_gc, aes(x=log(price)))+geom_density()
  #+end_src
  
  #+attr_latex :width 150px :height 150px
  #+RESULTS:
  [[file:dens2.png]]
* Transformations
- Useful to get a better sense of the data
- Have a bunch of assumptions (what's the log of -1)
- Can be used to deceive very, very easily
- Really really useful in everyday practice
* Getting the sense of things
- Picking the right visualisation for the data is important

#+begin_src R :session :results output graphics :file scatter_bad.png 
ggplot(dubcity, aes(x=sale_date, y=price))+geom_point()
#+end_src

#+attr_latex: :width 200px :height 200px
#+RESULTS:
[[file:scatter_bad.png]]

- is this a good plot?
- does this depend on the number of points?
* Sampling and Plotting

#+begin_src R :session :results output graphics :file scatter_bad2.png 
ggplot(dubcity_samp, aes(x=sale_date, y=price))+geom_point()
#+end_src
#+attr_latex :width 150px :height 150px
#+RESULTS:
[[file:scatter_bad2.png]]

- Not really
* Transformations Help 
  #+begin_src R :session :results output graphics :file logscatter.png :exports results
  ggplot(ppr_gc_smaller, aes(x=sale_date, y=log(price, 10)))+geom_point()
  #+end_src
  #+attr_latex :width 150px :height 150px
  #+RESULTS:
  [[file:logscatter.png]]

- Note the log 10 base
- Some of you may be able to convert from base 2.718, but I missed
  that class in school
- Still crap though
* No data is an island

- The first obvious thing is to split by county, right?
#+begin_src R :session :results output graphics :file scat_county1.png
ggplot(ppr_gc_smaller, aes(x=sale_date, y=log(price, 10)))+geom_point()+facet_wrap(~ppr_county)
#+end_src

#+attr_latex :width 150px :height 150px
#+RESULTS:
[[file:scat_county1.png]]
- Oh look, it's lot of little boxes of crap :(
* Summarisation
- The obvious answer is summarisation
#+begin_src R :session :exports both :results output graphics :file linecounty1.png
county_daily <- ppr_gc2 %>% group_by(sale_date, ppr_county, region) %>%
  summarise(count=n(), min_price=min(price),
            median_price=median(price),
            max_price=max(price)) %>%
  mutate(min_to_median=min_price/median_price,
         max_to_median=max_price/median_price,
         max_to_min=max_price/min_price)
ggplot(county_daily, aes(x=sale_date, y=median_price, colour=ppr_county))+geom_line()
#+end_src
#+attr_latex :width 150px :height 150px
#+RESULTS:
[[file:linecounty1.png]]

#+ATTR_LATEX height: 200px width:150px
#+RESULTS:
[[file:linecounty1.png]]
* Reducing Alpha kinda works...
  #+begin_src R :session :results output graphics :file linecounty2.png :exports results
  ggplot(county_daily, aes(x=sale_date, y=median_price, colour=ppr_county))+geom_line(alpha=0.3)
  #+end_src

  #+RESULTS:
  [[file:linecounty2.png]]

- But really just washes the whole thing out
* A redundant faceting variable
- We just group by a higher level variable
#+begin_src R :session :results output graphics :file linecounty3.png :exports results
ggplot(county_daily, aes(x=sale_date, y=median_price, colour=ppr_county))+geom_line()+facet_wrap(~region)
#+end_src

#+RESULTS:
[[file:linecounty3.png]]

- Much clearer :)
* WTF?
- This is one of the major advantages of visualisation:
  - it helps to (dis)confirm your assumptions
  - given that we have too many lines in the various groupings,we know
    that somethng has gone horribly wrong
  - in this case, it's a mismatch between two different types of data
* Iterating over data and visuals :noexport:
#+begin_src R :session :colnames yes
# this is an S4 object with geographical data 
shp <- readOGR("~/Dropbox/PPR/electoral_divisions_gps.shp") 
#the data slot contains a dataframe - countyname is the LEO's
with(shp@data, table(COUNTYNAME)) %>% as.data.frame() %>% arrange(desc(Freq)) %>% head(10)
#+end_src

#+RESULTS:
| COUNTYNAME      | Freq |
|-----------------+------|
| Cork County     |  324 |
| Galway County   |  214 |
| Kerry County    |  164 |
| Dublin City     |  162 |
| Mayo County     |  152 |
| Clare County    |  151 |
| Donegal County  |  149 |
| Limerick County |  135 |
| Wexford County  |  124 |
| Kilkenny County |  113 |
- These are the local electoral authorities
- These are from the geocoded points, so they should be somewhat better
- the PPR data is sometimes crazy wrong [fn:5]


#+begin_src R :session :results none :exports code
#NUTS3
county_region_map <- shp@data[,"COUNTYNAME", "NUTS3NAME"]
ppr_gc_county_fix <- ppr_gc2 %>%
  mutate(COUNTYNAME=ifelse(length(geo_county)==1, paste(geo_county, "County", sep=" "), geo_county))
  
#+end_src
* Distributions (i.e. boxplots)
  #+begin_src R :session :results output graphics :file boxplot1.png :exports both
  ggplot(ppr_gc2, aes(x=as.factor(year), y=price))+geom_boxplot()
  #+end_src
  #+attr_latex :width 150px :height 150px
  #+RESULTS:
  [[file:boxplot1.png]]
* Faceting, redux
  #+begin_src R :session :results output graphics :file boxplot2.png :exports results 
    ggplot(ppr_gc2, aes(x=as.factor(year), y=price))+geom_boxplot()+facet_wrap(~region)
  #+end_src
  #+attr_latex :width 150px :height 150px
  #+RESULTS:
  [[file:boxplot2.png]]

- This actually works (for me, at least)
- can you explain this to a sales-person?

  

* Distributions over Time, Redux
  #+begin_src R :session :results output graphics :file density_year.png :exports results :width 400 :height 400 :center yes
  ggplot(ppr_gc2, aes(x=log(price, 10), fill=region))+geom_density(alpha=0.3)+facet_wrap(~year)+xlab("log10_price")+theme(axis.text.x=element_text(angle=-45))
  #+end_src
  #+RESULTS:
  [[file:density_year.png]]
- This is much, much better
- I definitely don't think I'd try to explain it to a business/sales person
* Performative vs Presentation
- Two types of graphs:
  - for yourself
  - for other people  (and different audiences need different things)
* Performative Graphics
- These are used to help you understand a problem
- typically created in an iterative fashion
- often move from data transformation to visualisation and back agai
* How to visualise common types of data
- scatterplot
- line plot
- reversed line plot (time moves from RTL)
- box plot 
- reversed box plot
* Spatial vs Temporal
- line plots vs maps
- time versus space
- both provide insight into 
- pick one, difficult to do both

* Presentation Graphics

* Different Audiences/story
- To some extent, your job with presentation visualisations is to tell a story
- hopefully, it will be nuanced, but that isn't a requirement [fn:2]
- Often good to show smooths as opposed to raw data
- raw data is often ugly
- need for care here, as this should only be done where there is a
  clear effect
* Interactivity and Dashboards
- Can show both time and space
- for reporting, these are essential
- Much more effort from a software-engineering perspective [fn:3]
* Reporting
- Some times you need to repeat yourself
- Couple of ways of approaching this
  - Dashboards
  - Automated Reports
* Dashboards
- Lots of effort to set up correctly
- typically need a bunch of ETL to get data into correct format
- Low-maintenance once the original work is done
- Much more useful for business users 
* Automated Reports
- Less effort to get working (especially with Sweave, knitr and org/pandoc)
- A lot more effort to get working in a Python/SQL context
- More maintenance over time (someone needs to update the report)
* Principles of Reporting Visualisations
- Time view essential
- preferably forecasts, with results of previous forecasts
- allows 
- Simple, simple, simple
- One clear message (key metric or whatever)
- available material for those that want to dig deeper
* Footnotes

[fn:5] one wonders if that's deliberate 

[fn:4] please someone in the audience suggest a better idea 

[fn:1] anything really, but we're talking about images here. 

[fn:2] and in fact, it may be better to remove all nuance from the
presentation and provide a longer document with all the failed
approaches and hacking needed to actually reproduce your results

[fn:3] for me, at least


